{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0e85cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from json import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Layer, Lambda, Multiply\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9247433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc194509",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "img_rows, img_cols = 28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype(float)/255\n",
    "x_test = x_test.astype(float)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c3ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb50db",
   "metadata": {},
   "source": [
    "### Basic Convolutional Autoencoder with a middle encoding layer with 25 elements\n",
    "\n",
    "I've found that running images through an autoencoder and then doing the final prediction on the encoding layer leads to better generalizability. However, the network doesn't have much thought into it, because I'm not trying to win any competitions, but rather trying to illustrate a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5574872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:53:28.917517: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-27 12:53:28.917653: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 49)                28273     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1250      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 49)                1274      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 143,374\n",
      "Trainable params: 143,118\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:53:29.305762: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-27 12:53:29.306056: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:53:29.666429: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:53:53.621380: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0680 - val_loss: 0.0678\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0674 - val_loss: 0.0678\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0674 - val_loss: 0.0677\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0674 - val_loss: 0.0675\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0674 - val_loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14fa40a00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Flatten, Reshape, Conv2D, MaxPool2D, BatchNormalization, Dense, Conv2DTranspose\n",
    "\n",
    "\n",
    "input = Input(shape=(28, 28, 1))\n",
    "\n",
    "# Encoder\n",
    "e = Conv2D(32, (3, 3), activation='sigmoid')(input)\n",
    "e = MaxPool2D((2, 2))(e)\n",
    "e = Conv2D(64, (3, 3), activation='sigmoid')(e)\n",
    "e = MaxPool2D((2, 2))(e)\n",
    "e = Conv2D(64, (3, 3), activation='sigmoid')(e)\n",
    "l = Flatten()(e)\n",
    "l = Dense(49, activation='sigmoid')(l)\n",
    "l = Dense(25, activation='sigmoid')(l)\n",
    "\n",
    "#DECODER\n",
    "d = Dense(49, activation='sigmoid')(l)\n",
    "d = Reshape((7,7,1))(d)\n",
    "d = Conv2DTranspose(64,(3, 3), strides=2, activation='sigmoid', padding='same')(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Conv2DTranspose(64,(3, 3), strides=2, activation='sigmoid', padding='same')(d)\n",
    "d = BatchNormalization()(d)\n",
    "d = Conv2DTranspose(32,(3, 3), activation='sigmoid', padding='same')(d)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(d)\n",
    "\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, decoded)\n",
    "autoencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.fit(\n",
    "    x=x_train,\n",
    "    y=x_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada9d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:55:37.513704: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 803/3750 [=====>........................] - ETA: 37s - loss: 0.0676"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b7/hz5qz0l56c1558bmwbgrktt00000gn/T/ipykernel_46767/467787665.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the models have different lines to train, just so I can retrain to see if I can get better performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m autoencoder.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nn_optimization/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# the models have different lines to train, just so I can retrain to see if I can get better performance\n",
    "autoencoder.fit(\n",
    "    x=x_train,\n",
    "    y=x_train,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    validation_data=(x_test, x_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39764cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 49)                1274      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 58,107\n",
      "Trainable params: 57,851\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x169809a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "(2, 28, 28, 1)\n",
      "(2, 25)\n",
      "(2, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 12:56:12.299398: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-11-27 12:56:12.390254: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1698dd160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding = Model(input, l)\n",
    "output = embedding.predict(x_train[0:2])\n",
    "print(output.shape)\n",
    "\n",
    "def generate_decoder(autoencoder, layers, encoding_size = 25):\n",
    "    decoded_output = Input(shape=(encoding_size,))\n",
    "    deco = autoencoder.layers[-1 * layers](decoded_output)\n",
    "    for i in range(layers - 1, 0, -1):\n",
    "        deco = autoencoder.layers[-1 * i](deco)\n",
    "    decoding = Model(decoded_output, deco)\n",
    "    decoding.summary()\n",
    "    return(decoding)\n",
    "decoder = generate_decoder(autoencoder, 8, 25)\n",
    "reconstruction = decoder.predict(output)\n",
    "print(x_train[0:2].shape)\n",
    "print(output.shape)\n",
    "print(reconstruction.shape)\n",
    "#plt.imshow(x_train[0])\n",
    "\n",
    "plt.imshow(decoder.predict(output)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "199a6e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 30)                780       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,610\n",
      "Trainable params: 1,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 18:38:46.064285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  12/1875 [..............................] - ETA: 18s - loss: 2.3295 - accuracy: 0.0547"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 18:38:50.538834: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.7592 - accuracy: 0.7805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 18:39:07.568750: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.7587 - accuracy: 0.7807 - val_loss: 0.4005 - val_accuracy: 0.8817\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3705 - accuracy: 0.8874 - val_loss: 0.3386 - val_accuracy: 0.9001\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3354 - accuracy: 0.8979 - val_loss: 0.3194 - val_accuracy: 0.9051\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3161 - accuracy: 0.9025 - val_loss: 0.3179 - val_accuracy: 0.9039\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2996 - accuracy: 0.9078 - val_loss: 0.2845 - val_accuracy: 0.9135\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.2847 - accuracy: 0.9115 - val_loss: 0.2604 - val_accuracy: 0.9195\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2704 - accuracy: 0.9165 - val_loss: 0.2473 - val_accuracy: 0.9242\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2564 - accuracy: 0.9194 - val_loss: 0.2437 - val_accuracy: 0.9272\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2420 - accuracy: 0.9251 - val_loss: 0.2209 - val_accuracy: 0.9315\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2297 - accuracy: 0.9296 - val_loss: 0.2239 - val_accuracy: 0.9331\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2189 - accuracy: 0.9327 - val_loss: 0.2050 - val_accuracy: 0.9393\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2094 - accuracy: 0.9356 - val_loss: 0.1968 - val_accuracy: 0.9383\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2005 - accuracy: 0.9377 - val_loss: 0.1842 - val_accuracy: 0.9446\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1939 - accuracy: 0.9404 - val_loss: 0.1821 - val_accuracy: 0.9445\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1870 - accuracy: 0.9429 - val_loss: 0.1779 - val_accuracy: 0.9465\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1815 - accuracy: 0.9446 - val_loss: 0.1688 - val_accuracy: 0.9505\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1759 - accuracy: 0.9462 - val_loss: 0.1676 - val_accuracy: 0.9514\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1716 - accuracy: 0.9480 - val_loss: 0.1648 - val_accuracy: 0.9493\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1675 - accuracy: 0.9491 - val_loss: 0.1591 - val_accuracy: 0.9520\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1632 - accuracy: 0.9505 - val_loss: 0.1572 - val_accuracy: 0.9539\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1592 - accuracy: 0.9517 - val_loss: 0.1504 - val_accuracy: 0.9537\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1558 - accuracy: 0.9534 - val_loss: 0.1453 - val_accuracy: 0.9551\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1521 - accuracy: 0.9540 - val_loss: 0.1392 - val_accuracy: 0.9571\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1497 - accuracy: 0.9545 - val_loss: 0.1442 - val_accuracy: 0.9558\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1456 - accuracy: 0.9560 - val_loss: 0.1553 - val_accuracy: 0.9520\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1439 - accuracy: 0.9567 - val_loss: 0.1389 - val_accuracy: 0.9572\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1405 - accuracy: 0.9575 - val_loss: 0.1321 - val_accuracy: 0.9595\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1378 - accuracy: 0.9581 - val_loss: 0.1387 - val_accuracy: 0.9590\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1349 - accuracy: 0.9596 - val_loss: 0.1260 - val_accuracy: 0.9620\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1332 - accuracy: 0.9602 - val_loss: 0.1308 - val_accuracy: 0.9613\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1310 - accuracy: 0.9605 - val_loss: 0.1293 - val_accuracy: 0.9615\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1293 - accuracy: 0.9613 - val_loss: 0.1297 - val_accuracy: 0.9615\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1277 - accuracy: 0.9620 - val_loss: 0.1195 - val_accuracy: 0.9647\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1269 - accuracy: 0.9620 - val_loss: 0.1184 - val_accuracy: 0.9640\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1245 - accuracy: 0.9620 - val_loss: 0.1191 - val_accuracy: 0.9650\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1228 - accuracy: 0.9632 - val_loss: 0.1157 - val_accuracy: 0.9659\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1217 - accuracy: 0.9632 - val_loss: 0.1230 - val_accuracy: 0.9642\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1206 - accuracy: 0.9643 - val_loss: 0.1170 - val_accuracy: 0.9660\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1190 - accuracy: 0.9643 - val_loss: 0.1165 - val_accuracy: 0.9656\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1175 - accuracy: 0.9637 - val_loss: 0.1249 - val_accuracy: 0.9636\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1165 - accuracy: 0.9649 - val_loss: 0.1105 - val_accuracy: 0.9673\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1158 - accuracy: 0.9649 - val_loss: 0.1126 - val_accuracy: 0.9680\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1147 - accuracy: 0.9656 - val_loss: 0.1155 - val_accuracy: 0.9662\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1132 - accuracy: 0.9664 - val_loss: 0.1142 - val_accuracy: 0.9655\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1127 - accuracy: 0.9656 - val_loss: 0.1107 - val_accuracy: 0.9681\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1126 - accuracy: 0.9670 - val_loss: 0.1066 - val_accuracy: 0.9684\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1111 - accuracy: 0.9665 - val_loss: 0.1191 - val_accuracy: 0.9623\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1102 - accuracy: 0.9668 - val_loss: 0.1133 - val_accuracy: 0.9673\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1100 - accuracy: 0.9667 - val_loss: 0.1083 - val_accuracy: 0.9692\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1093 - accuracy: 0.9673 - val_loss: 0.1076 - val_accuracy: 0.9679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a143550>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "input_layer = Input(shape=(25,))\n",
    "x = Dense(30, activation=\"elu\")(input_layer)\n",
    "x = Dense(20, activation=\"elu\")(x)\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "prediction_model = Model(input_layer, output_layer)\n",
    "prediction_model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "prediction_model.summary()\n",
    "\n",
    "x_train_embedding = embedding.predict(x_train)\n",
    "x_test_embedding = embedding.predict(x_test)\n",
    "\n",
    "prediction_model.fit(x_train_embedding, y_train, batch_size = batch_size, epochs = epochs, \n",
    "          validation_data = (x_test_embedding, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90c702c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model_layers = embedding.layers[-1].output\n",
    "jml = prediction_model.layers[-3](joint_model_layers)\n",
    "jml = prediction_model.layers[-2](jml)\n",
    "jml = prediction_model.layers[-1](jml)\n",
    "\n",
    "joint_model = Model(embedding.input, jml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "883f592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_layer(Layer):\n",
    "    def  __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(custom_layer,self).__init__(**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        output_shape = self.compute_output_shape(input_shape)\n",
    "        self.W=self.add_weight(name='kernel',\n",
    "                           shape=(1,) + output_shape[1:],\n",
    "                           constraint=tf.keras.constraints.NonNeg(),\n",
    "                           initializer='uniform'                              ,\n",
    "                           trainable=True)\n",
    "        self.built = True\n",
    "  # this self.built is necessary .\n",
    "    def call(self,x):\n",
    "        return x * self.W\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ec4cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "custom_layer_2 (custom_layer (None, 28, 28, 1)         784       \n",
      "_________________________________________________________________\n",
      "model_33 (Functional)        (None, 10)                86877     \n",
      "=================================================================\n",
      "Total params: 87,661\n",
      "Trainable params: 784\n",
      "Non-trainable params: 86,877\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "  1/625 [..............................] - ETA: 3:00 - loss: 10.9941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-27 08:25:30.441063: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 7s 11ms/step - loss: 0.0744\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.0017\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 0.0011\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.1137e-04\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 6.5505e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x36059a9a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Layer, Lambda, Multiply, Reshape\n",
    "\n",
    "sample_input = np.ones(10000 * 28 * 28).reshape(10000, 28, 28)\n",
    "sample_output = np.array(10000 * [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])\n",
    "\n",
    "\n",
    "joint_model.trainable = False\n",
    "\n",
    "new_input_layer =  Input(shape=(28,28,1))\n",
    "l = custom_layer((28,28,1))\n",
    "# this is the layer whose weight we believe is going to be the minimum\n",
    "new_weight_layer = l(new_input_layer)\n",
    "transfer = joint_model(new_weight_layer, training=False)\n",
    "\n",
    "\n",
    "optimization_model = Model(new_input_layer, transfer)\n",
    "optimization_model.compile('sgd', loss = 'categorical_crossentropy')\n",
    "optimization_model.summary()\n",
    "# basic training framework that is used in keras\n",
    "optimization_model.fit(sample_input, sample_output, batch_size=16, epochs=5, verbose = 1)\n",
    "# the model that we trained previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb22be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.1338e-04\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.1073e-04\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.0823e-04\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.0584e-04\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.0352e-04\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 1.0126e-04\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.9158e-05\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.7185e-05\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.5278e-05\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.3455e-05\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.1699e-05\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 9.0020e-05\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.8366e-05\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.6742e-05\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.5247e-05\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.3785e-05\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.2393e-05\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 8.1031e-05\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.9740e-05\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.8471e-05\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.7273e-05\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.6086e-05\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.4932e-05\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.3827e-05\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.2638e-05\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.1587e-05\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 7.0558e-05\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 6.9566e-05\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 6.8613e-05\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 7s 11ms/step - loss: 6.7685e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x300cb30a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization_model.fit(sample_input, sample_output, batch_size=16, epochs=30, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bdb31ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1610277e-06 1.9678517e-07 4.0928233e-02 7.2253577e-05 2.1368247e-02\n",
      "  3.0442451e-03 9.2240971e-01 1.2154309e-02 3.4262089e-07 1.7279577e-05]]\n",
      "[[1.3401997e-07 4.8629930e-14 6.0651240e-07 6.1211752e-10 1.6491877e-05\n",
      "  4.8702739e-05 9.9993289e-01 1.2919630e-06 4.4041271e-11 3.0883163e-09]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "joint_model.predict(l.get_weights())\n",
    "\n",
    "plt.imshow(l.get_weights()[0].reshape(28,28))\n",
    "plt.show()\n",
    "img = autoencoder.predict(l.get_weights())[0].reshape(28, 28)\n",
    "plt.imshow(img)\n",
    "print(joint_model.predict(img.reshape(1 ,28, 28, 1)))\n",
    "print(joint_model.predict(l.get_weights()[0].reshape(1, 28,28, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3731c918",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 784 into shape (1,49)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b7/hz5qz0l56c1558bmwbgrktt00000gn/T/ipykernel_43251/523613292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 784 into shape (1,49)"
     ]
    }
   ],
   "source": [
    "img = np.array(l.get_weights())\n",
    "img = img.reshape(1, 49)\n",
    "new_output = decoding.predict(img)\n",
    "\n",
    "prediction_model.predict(l.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be5b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 13, 13, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 5, 5, 16)          64        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 16)          2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 1, 16)          64        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 5,162\n",
      "Trainable params: 5,066\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dense\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(28,28,1))\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='sigmoid', input_shape = (28, 28, 1))(input_layer)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='sigmoid')(x)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='relu')(x)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8bde58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 13, 13, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 5, 5, 16)          64        \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 3, 3, 16)          2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 1, 16)          64        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 5,162\n",
      "Trainable params: 5,066\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dense\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(28,28,1))\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='sigmoid', input_shape = (28, 28, 1))(input_layer)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='sigmoid')(x)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(filters = 16, kernel_size = (3, 3), activation='relu')(x)\n",
    "x = MaxPool2D(strides=(2,2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "output_layer = Dense(10, activation='softmax')(x)\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa63d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3eb294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "118/118 [==============================] - 3s 23ms/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 0.0726 - val_accuracy: 0.9781\n",
      "Epoch 2/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.0732 - val_accuracy: 0.9768\n",
      "Epoch 3/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0478 - accuracy: 0.9855 - val_loss: 0.0716 - val_accuracy: 0.9800\n",
      "Epoch 4/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0444 - accuracy: 0.9868 - val_loss: 0.0676 - val_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "118/118 [==============================] - 3s 21ms/step - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0720 - val_accuracy: 0.9773\n",
      "Epoch 6/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 0.0649 - val_accuracy: 0.9809\n",
      "Epoch 7/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0374 - accuracy: 0.9887 - val_loss: 0.0790 - val_accuracy: 0.9772\n",
      "Epoch 8/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0344 - accuracy: 0.9900 - val_loss: 0.0736 - val_accuracy: 0.9781\n",
      "Epoch 9/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0327 - accuracy: 0.9902 - val_loss: 0.0719 - val_accuracy: 0.9780\n",
      "Epoch 10/10\n",
      "118/118 [==============================] - 3s 22ms/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 0.0748 - val_accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e7e19a60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, \n",
    "          validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26970b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_layer(Layer):\n",
    "    def  __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(custom_layer,self).__init__(**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        output_shape = self.compute_output_shape(input_shape)\n",
    "        self.W=self.add_weight(name='kernel',\n",
    "                           shape=(1,) + output_shape[1:],\n",
    "                           constraint=tf.keras.constraints.NonNeg(),\n",
    "                           initializer='uniform'                              ,\n",
    "                           trainable=True)\n",
    "        self.built = True\n",
    "  # this self.built is necessary .\n",
    "    def call(self,x):\n",
    "        return x * self.W\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72550e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "sample_input = np.ones(10000 * 28 * 28).reshape(10000, 28, 28)\n",
    "sample_output = np.array(10000 * [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "sample_output.shape\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "909b95be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "custom_layer (custom_layer)  (None, 28, 28, 1)         784       \n",
      "_________________________________________________________________\n",
      "model_3 (Functional)         (None, 10)                5162      \n",
      "=================================================================\n",
      "Total params: 5,946\n",
      "Trainable params: 784\n",
      "Non-trainable params: 5,162\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      " 3/10 [========>.....................] - ETA: 0s - loss: 6.2393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 16:22:33.549852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 32ms/step - loss: 5.6989\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 4.0493\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 1.5508\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.3342\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0990\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0564\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0406\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0320\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0266\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0230\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0204\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0182\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0165\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0152\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0140\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0129\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0120\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0113\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0106\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0100\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0094\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0089\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0084\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0080\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0077\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0073\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0070\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0067\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0065\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0063\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0061\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0059\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0057\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0055\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0053\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0052\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0050\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0049\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0048\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0046\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0045\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0044\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0043\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0042\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0041\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0040\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0039\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0039\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 0.0038\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e7e195b0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Layer, Lambda, Multiply, Reshape\n",
    "model.trainable = False\n",
    "def my_loss_fn(y_true, y_pred):\n",
    "    return  (1 - y_pred[0]) + tf.reduce_mean(y_pred, axis=-1) \n",
    "\n",
    "new_input_layer =  Input(shape=(28,28,1))\n",
    "l = custom_layer((28,28,1))\n",
    "# this is the layer whose weight we believe is going to be the minimum\n",
    "new_weight_layer = l(new_input_layer)\n",
    "transfer = model(new_weight_layer, training=False)\n",
    "\n",
    "\n",
    "optimization_model = Model(new_input_layer, transfer)\n",
    "optimization_model.compile('sgd', loss = 'categorical_crossentropy')\n",
    "optimization_model.summary()\n",
    "# basic training framework that is used in keras\n",
    "optimization_model.fit(sample_input, sample_output, batch_size=1024, epochs=50, verbose = 1)\n",
    "# the model that we trained previously\n",
    "#transfer = model(new_weight_layer, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_model.fit(sample_input, sample_output, batch_size=8, epochs=50, verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52317e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = np.array(l.get_weights()).reshape(( 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db87a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.8372141e-06 1.7723161e-05 9.9993908e-01 2.4317018e-05 5.1481091e-09\n",
      "  2.7174497e-06 4.0945881e-07 7.9448027e-06 7.3929620e-08 4.8044275e-09]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 16:27:29.490902: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d8f62850>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.predict(img.reshape(1,28,28,1)))\n",
    "img = img.reshape(28,28)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bad7d634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5545107"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70657a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
